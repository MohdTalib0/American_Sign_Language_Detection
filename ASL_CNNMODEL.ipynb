{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK OF CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Dense,Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkConstants():  # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Constant values used as image and network parameters.\"\"\"\n",
    "\n",
    "    # Width of images passed to the network\n",
    "    IMAGE_WIDTH: int = 200\n",
    "\n",
    "    # Height of images passed to the network\n",
    "    IMAGE_HEIGHT: int = 200\n",
    "\n",
    "    # Currently set to 2 alphabet images and 1 background image class\n",
    "    # Number of classes that the network can categorize\n",
    "    NUM_CLASSES: int = 27\n",
    "\n",
    "    # The fraction of images passed to the network during training that should\n",
    "    # be used as a validation set. Range: 0 to 1\n",
    "    VALIDATION_SPLIT: float = 0.1\n",
    "\n",
    "    # The fraction of images passed to the network during training that should\n",
    "    # be used as a test set. Range: 0 to 1\n",
    "    TEST_SPLIT: float = 0.2\n",
    "\n",
    "    # Number of epochs on which to train the network\n",
    "    EPOCHS: int = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "        images = []\n",
    "        labels = []\n",
    "        dataset_root_folder = \"signlanguage\"\n",
    "        for filename in os.listdir(dataset_root_folder):\n",
    "            if filename.endswith(\".png\") and not filename.startswith(\".\"):\n",
    "                # Read black and white image\n",
    "                image = Image.open(os.path.join(dataset_root_folder, filename))\n",
    "                # Convert image to an array with shape (image_width, image_height, 1)\n",
    "                image_data = img_to_array(image)\n",
    "                images.append(image_data)\n",
    "\n",
    "                label = filename[0]\n",
    "                if filename.startswith(\"background\"):\n",
    "                    # Use the last class to denote an unknown/background image\n",
    "                    label = NetworkConstants.NUM_CLASSES - 1\n",
    "                else:\n",
    "                    # Use ordinal value offsets to denote labels for all alphabets\n",
    "                    label = ord(label) - 97\n",
    "                labels.append(label)\n",
    "\n",
    "        # Normalize the image data\n",
    "        images = np.array(images, dtype=\"float32\") / 255.0\n",
    "        # Convert labels to a numpy array\n",
    "        labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 26])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images, test_images, training_labels, test_labels = train_test_split(images, labels, test_size=NetworkConstants.TEST_SPLIT)\n",
    "        # Convert array of labels in to binary classs matrix\n",
    "#training_labels = keras.utils.to_categorical(training_labels, num_classes=NetworkConstants.NUM_CLASSES)\n",
    "#test_labels = keras.utils.to_categorical(test_labels, num_classes=NetworkConstants.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 198, 198, 64)      640       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 99, 99, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 99, 99, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 97, 97, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 48, 48, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 48, 48, 128)       0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 46, 46, 512)       590336    \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 23, 23, 512)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 23, 23, 512)       0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 270848)            0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 270848)            0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               138674688 \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 27)                13851     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,353,371\n",
      "Trainable params: 139,353,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),input_shape=(200,200,1),activation='relu'))\n",
    "#model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\"))\n",
    "#model.add(Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3),activation=\"relu\"))\n",
    "#model.add(Conv2D(filters=512,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=512,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=27,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 889s 5s/step - loss: 2.1270 - accuracy: 0.3833 - val_loss: 0.8523 - val_accuracy: 0.7670\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 845s 4s/step - loss: 0.6453 - accuracy: 0.8079 - val_loss: 0.3069 - val_accuracy: 0.9322\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 877s 5s/step - loss: 0.2916 - accuracy: 0.9076 - val_loss: 0.1535 - val_accuracy: 0.9602\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 997s 5s/step - loss: 0.1919 - accuracy: 0.9397 - val_loss: 0.1244 - val_accuracy: 0.9646\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 1012s 5s/step - loss: 0.1361 - accuracy: 0.9579 - val_loss: 0.1343 - val_accuracy: 0.9661\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 1049s 5s/step - loss: 0.1072 - accuracy: 0.9649 - val_loss: 0.0910 - val_accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 1008s 5s/step - loss: 0.0805 - accuracy: 0.9748 - val_loss: 0.0729 - val_accuracy: 0.9794\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 850s 4s/step - loss: 0.0577 - accuracy: 0.9805 - val_loss: 0.1041 - val_accuracy: 0.9794\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 774s 4s/step - loss: 0.0722 - accuracy: 0.9795 - val_loss: 0.0982 - val_accuracy: 0.9794\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 796s 4s/step - loss: 0.0610 - accuracy: 0.9816 - val_loss: 0.0821 - val_accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d045a0c0a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_images,training_labels,epochs=10,verbose=1,validation_split=NetworkConstants.VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ASL_CNN_MODEL1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump (model, open (\"ASL_final\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
